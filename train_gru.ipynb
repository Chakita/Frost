{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a Gated Reccurent Unit for text generation.The model is seeded with the last word of the sentence and it tries to predict the preceeding words. The word embeddings produced by the word2vec model is used in the embedding layer of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reverse_sentence(sentence):\n",
    "\twords=sentence.split()\n",
    "\twords=words[::-1]\n",
    "\tnew_sent=\" \".join(words)\n",
    "\treturn new_sent\n",
    "\n",
    "training_file=open(\"taylorswift.txt\")\n",
    "poems1=open(\"keats.txt\")\n",
    "poems2=open(\"frost_poems.txt\")\n",
    "corpus1=training_file.read().lower().split(\"\\n\")\n",
    "corpus1=[sentence for sentence in corpus1 if(sentence!='' and len(sentence)>1)]\n",
    "corpus2=poems1.read().lower().split(\"\\n\")\n",
    "corpus2=[sentence for sentence in corpus2 if(sentence!='' and len(sentence)>1)]\n",
    "corpus3=poems2.read().lower().split(\"\\n\")\n",
    "corpus3=[sentence for sentence in corpus3 if(sentence!='' and len(sentence)>1)]\n",
    "corpus=corpus1+corpus2+corpus3\n",
    "#Since the model generates sentences backwards,starting from the last word, it is trained on reversed sentences\n",
    "corpus=list(map(reverse_sentence,corpus))\n",
    "words=[[word for word in sentence.split()] for sentence in corpus]\n",
    "lengths=[len(sentence) for sentence in corpus]\n",
    "max_len=max(lengths)\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,GRU,Dense,Bidirectional,Attention,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word_model=Word2Vec.load(\"word2vec_model\")\n",
    "pretrained_weights=word_model.wv.syn0\n",
    "vocab_size,embedding_size=pretrained_weights.shape\n",
    "\n",
    "def word2index(word):\n",
    "\treturn word_model.wv.vocab[word].index\n",
    "def index_to_word(index):\n",
    "\treturn word_model.wv.index2word[index]\t\n",
    "\t\n",
    "xs=np.zeros([len(words),max_len],dtype=np.int32)\n",
    "ys=np.zeros([len(words)],dtype=np.int32)\n",
    "for i,sent in enumerate(words):\n",
    "\tif sent:\n",
    "\t\tfor t,word in enumerate(sent[:-1]):\n",
    "\t\t\txs[i,t]=word2index(word)\n",
    "\t\tys[i]=word2index(sent[-1])\n",
    "\t\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,output_dim=embedding_size,weights=[pretrained_weights]))\n",
    "model.add(GRU(units=embedding_size,return_sequences=True))\n",
    "model.add(GRU(units=embedding_size))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(units=vocab_size,activation=\"softmax\"))\n",
    "adam=Adam(lr=0.01)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=adam,metrics=[\"SparseCategoricalAccuracy\"])\n",
    "history=model.fit(xs,ys,batch_size=128,epochs=100,verbose=1)\n",
    "model.save(\"poet_gru_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
